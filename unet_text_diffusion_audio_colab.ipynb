{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81dabdb",
   "metadata": {},
   "source": [
    "# Shared 1D U-Net text+image+audio on Colab\n",
    "Optimized for free-tier T4 GPU usage\n",
    "- Reduced batch size\n",
    "- Time-based checkpointing every 25 minutes\n",
    "- Retains only the latest 3 checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ef642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006b8edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a0b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, time\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Hyperparameters\n",
    "SEQ_LEN     = 64\n",
    "IMG_SIZE    = 64\n",
    "PATCH       = 8\n",
    "DIFF_STEPS  = 8\n",
    "BATCH_SIZE  = 2   # Reduced for free-tier memory limits\n",
    "LR          = 3e-4\n",
    "EMB_DIM     = 512\n",
    "CHANNELS    = (512, 768, 1024)\n",
    "EPOCHS      = 1\n",
    "MASK, PAD   = \"<mask>\", \"<pad>\"\n",
    "DEVICE      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IGNORE_IDX  = -100\n",
    "BETA_START  = 1e-4\n",
    "BETA_END    = 0.02\n",
    "\n",
    "# Audio config\n",
    "SR          = 16000\n",
    "N_MELS      = 64\n",
    "HOP_LENGTH  = 256\n",
    "WIN_LENGTH  = 1024\n",
    "\n",
    "# Checkpointing settings\n",
    "checkpoint_dir = '/content/drive/MyDrive/checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Time-based checkpoint interval (seconds)\n",
    "last_save_time = time.time()\n",
    "save_interval = 25 * 60  # 25 minutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d84b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Diffusion schedule ---\n",
    "betas      = torch.linspace(BETA_START, BETA_END, DIFF_STEPS, device=DEVICE)\n",
    "alphas     = 1 - betas\n",
    "alphas_cum = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "def q_sample_text(x, t, mask_id):\n",
    "    rate = (t.float()+1)/DIFF_STEPS\n",
    "    m = torch.rand_like(rate) < rate\n",
    "    y = x.clone(); y[m] = mask_id\n",
    "    return y\n",
    "\n",
    "def q_sample_img(x, t):\n",
    "    out = []\n",
    "    for ti, xi in zip(t, x):\n",
    "        a = alphas_cum[ti].sqrt()\n",
    "        noise = torch.randn_like(xi)\n",
    "        out.append(a*xi + (1-a)*noise)\n",
    "    return torch.stack(out)\n",
    "\n",
    "def q_sample_audio(x, t):\n",
    "    out = []\n",
    "    for ti, xi in zip(t, x):\n",
    "        a = alphas_cum[ti].sqrt()\n",
    "        noise = torch.randn_like(xi)\n",
    "        out.append(a*xi + (1-a)*noise)\n",
    "    return torch.stack(out)\n",
    "\n",
    "# --- Tokenizer & Datasets ---\n",
    "class WordTokenizer:\n",
    "    def __init__(self, vocab, specials=(PAD, MASK)):\n",
    "        self.t2i = {tok:i for i,tok in enumerate(specials)}\n",
    "        for w in vocab:\n",
    "            if w not in self.t2i:\n",
    "                self.t2i[w] = len(self.t2i)\n",
    "        self.i2t = {i:tok for tok,i in self.t2i.items()}\n",
    "        self.vsz  = len(self.t2i)\n",
    "        self.pad  = self.t2i[PAD]\n",
    "        self.mask = self.t2i[MASK]\n",
    "    def encode(self, txt):\n",
    "        ids = [self.t2i.get(w, self.pad) for w in txt.strip().split()]\n",
    "        ids = (ids + [self.pad]*SEQ_LEN)[:SEQ_LEN]\n",
    "        return torch.tensor(ids, dtype=torch.long)\n",
    "    def decode(self, ids):\n",
    "        return \" \".join(self.i2t[i] for i in ids if i != self.pad)\n",
    "\n",
    "def build_vocab(src, max_size=50000):\n",
    "    from collections import Counter\n",
    "    cnt = Counter()\n",
    "    paths = ([src] if os.path.isfile(src)\n",
    "             else [os.path.join(dp,f)\n",
    "                   for dp,_,fs in os.walk(src) for f in fs if f.endswith('.txt')])\n",
    "    for p in paths:\n",
    "        with open(p, encoding='utf8', errors='ignore') as f:\n",
    "            for ln in f:\n",
    "                cnt.update(ln.split())\n",
    "    return [w for w,_ in cnt.most_common(max_size) if w not in (MASK, PAD)]\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, root, tok):\n",
    "        self.samples = []\n",
    "        for dp,_,fs in os.walk(root):\n",
    "            for fn in fs:\n",
    "                if not fn.endswith('.txt'): continue\n",
    "                words = open(os.path.join(dp,fn), encoding='utf8').read().split()\n",
    "                for i in range(0, len(words), SEQ_LEN):\n",
    "                    chunk = words[i:i+SEQ_LEN]\n",
    "                    if len(chunk) < SEQ_LEN:\n",
    "                        chunk += [PAD]*(SEQ_LEN - len(chunk))\n",
    "                    ids = [tok.t2i.get(w, tok.pad) for w in chunk]\n",
    "                    self.samples.append(torch.tensor(ids, dtype=torch.long))\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx): return self.samples[idx]\n",
    "\n",
    "def pil_transform(img: Image.Image):\n",
    "    img = img.resize((IMG_SIZE, IMG_SIZE), Image.LANCZOS)\n",
    "    arr = np.asarray(img).astype(np.float32) / 255.0\n",
    "    arr = (arr - 0.5) / 0.5\n",
    "    return torch.from_numpy(arr).permute(2,0,1)\n",
    "\n",
    "class AllImagesDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        exts = ('*.png','*.jpg','*.jpeg','*.bmp','*.gif')\n",
    "        self.paths = sum((glob.glob(os.path.join(root,'**',e), recursive=True) for e in exts), [])\n",
    "        if not self.paths:\n",
    "            raise RuntimeError(f\"No images in {root!r}\")\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert('RGB')\n",
    "        return pil_transform(img), 0\n",
    "\n",
    "def get_image_loader(root):\n",
    "    return DataLoader(AllImagesDataset(root), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "class AllAudioDataset(Dataset):\n",
    "    def __init__(self, root):\n",
    "        exts = ('*.wav','*.mp3','*.flac')\n",
    "        self.paths = sum((glob.glob(os.path.join(root,'**',e), recursive=True) for e in exts), [])\n",
    "        if not self.paths:\n",
    "            raise RuntimeError(f\"No audio in {root!r}\")\n",
    "    def __len__(self): return len(self.paths)\n",
    "    def __getitem__(self, idx):\n",
    "        wav, sr = librosa.load(self.paths[idx], sr=SR, mono=True)\n",
    "        if wav.shape[0] < WIN_LENGTH:\n",
    "            wav = np.pad(wav, (0, WIN_LENGTH - wav.shape[0]), mode='constant')\n",
    "        spec = librosa.feature.melspectrogram(\n",
    "            y=wav, sr=SR,\n",
    "            n_fft=WIN_LENGTH,\n",
    "            hop_length=HOP_LENGTH,\n",
    "            win_length=WIN_LENGTH,\n",
    "            n_mels=N_MELS,\n",
    "        )\n",
    "        spec = torch.from_numpy(spec).float()\n",
    "        spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "        spec = spec.T\n",
    "        if spec.shape[0] < SEQ_LEN:\n",
    "            spec = F.pad(spec, (0,0,0, SEQ_LEN-spec.shape[0]))\n",
    "        return spec, 0\n",
    "\n",
    "def get_audio_loader(root):\n",
    "    return DataLoader(AllAudioDataset(root), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "\n",
    "# --- Model definitions ---\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, ch, emb):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(emb, ch*2)\n",
    "    def forward(self, x, temb):\n",
    "        g,b = self.fc(temb).chunk(2, dim=-1)\n",
    "        return x * (1+g.unsqueeze(-1)) + b.unsqueeze(-1)\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    def __init__(self, channels, emb_dim, kernel=3, groups=8):\n",
    "        super().__init__()\n",
    "        p = kernel//2\n",
    "        self.conv1 = nn.Conv1d(channels,channels,kernel,padding=p)\n",
    "        self.conv2 = nn.Conv1d(channels,channels,kernel,padding=p)\n",
    "        self.norm1 = nn.GroupNorm(groups,channels)\n",
    "        self.norm2 = nn.GroupNorm(groups,channels)\n",
    "        self.act   = nn.GELU()\n",
    "        self.film  = FiLM(channels,emb_dim)\n",
    "    def forward(self, x, temb):\n",
    "        h = self.conv1(self.act(self.norm1(x)))\n",
    "        h = self.film(h, temb)\n",
    "        h = self.conv2(self.act(self.norm2(h)))\n",
    "        if h.size(-1) != x.size(-1):\n",
    "            L = min(h.size(-1), x.size(-1))\n",
    "            h, x = h[...,:L], x[...,:L]\n",
    "        return x + h\n",
    "\n",
    "class TokenUNet(nn.Module):\n",
    "    def __init__(self, emb_dim, channels, steps):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Embedding(steps+1, emb_dim),\n",
    "            nn.SiLU(), nn.Linear(emb_dim, emb_dim*4),\n",
    "            nn.SiLU(), nn.Linear(emb_dim*4, emb_dim)\n",
    "        )\n",
    "        prev = emb_dim\n",
    "        self.down = nn.ModuleList()\n",
    "        for ch in channels:\n",
    "            self.down.append(nn.ModuleDict({\n",
    "                'res': ResBlock1D(prev, emb_dim),\n",
    "                'down': nn.Conv1d(prev, ch, 4, 2, 1)\n",
    "            }))\n",
    "            prev = ch\n",
    "        self.mid = ResBlock1D(prev, emb_dim)\n",
    "        self.up  = nn.ModuleList()\n",
    "        for ch in reversed(channels):\n",
    "            self.up.append(nn.ModuleDict({\n",
    "                'up': nn.ConvTranspose1d(prev, ch, 4, 2, 1),\n",
    "                'res': ResBlock1D(ch, emb_dim)\n",
    "            }))\n",
    "            prev = ch\n",
    "        self.norm = nn.GroupNorm(8, emb_dim)\n",
    "    def forward(self, x, t):\n",
    "        temb = self.time_mlp(t)\n",
    "        skips=[]; h=x\n",
    "        for blk in self.down:\n",
    "            h = blk['res'](h, temb); skips.append(h)\n",
    "            h = blk['down'](h)\n",
    "        h = self.mid(h, temb)\n",
    "        for blk in self.up:\n",
    "            h = blk['up'](h)\n",
    "            skip = skips.pop()\n",
    "            if skip.size(-1) != h.size(-1):\n",
    "                L = min(skip.size(-1), h.size(-1))\n",
    "                skip, h = skip[...,:L], h[...,:L]\n",
    "            if skip.size(1) != h.size(1):\n",
    "                adapt = nn.Conv1d(skip.size(1), h.size(1), 1).to(skip.device)\n",
    "                skip = adapt(skip)\n",
    "            h = blk['res'](h + skip, temb)\n",
    "        return self.norm(h)\n",
    "\n",
    "class MMDiffuser(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_emb   = nn.Embedding(vocab_size, EMB_DIM)\n",
    "        self.patch_embed = nn.Conv2d(3, EMB_DIM, PATCH, PATCH)\n",
    "        self.audio_proj  = nn.Conv1d(N_MELS, EMB_DIM, 1)\n",
    "        self.unet        = TokenUNet(EMB_DIM, CHANNELS, DIFF_STEPS)\n",
    "        self.text_out    = nn.Linear(EMB_DIM, vocab_size)\n",
    "        self.patch_out   = nn.Conv1d(EMB_DIM, 3*PATCH*PATCH, 1)\n",
    "        self.audio_out   = nn.Conv1d(EMB_DIM, N_MELS, 1)\n",
    "\n",
    "    def forward_text(self, toks, t):\n",
    "        x = self.token_emb(toks).transpose(1,2)\n",
    "        h = self.unet(x,t).transpose(1,2)\n",
    "        return F.log_softmax(self.text_out(h), dim=-1)\n",
    "\n",
    "    def forward_img(self, imgs, t):\n",
    "        p   = self.patch_embed(imgs)\n",
    "        seq = p.view(p.size(0), p.size(1), -1)\n",
    "        h   = self.unet(seq, t)\n",
    "        return self.patch_out(h)\n",
    "\n",
    "    def forward_audio(self, spec, t):\n",
    "        x = spec.transpose(1,2)\n",
    "        x = self.audio_proj(x)\n",
    "        h = self.unet(x, t)\n",
    "        return self.audio_out(h)\n",
    "\n",
    "def save_checkpoint(model, vocab, checkpoint_dir):\n",
    "    timestamp = int(time.time())\n",
    "    path = os.path.join(checkpoint_dir, f\"mm_audio_{timestamp}.pt\")\n",
    "    torch.save({\"state_dict\": model.state_dict(), \"vocab\": vocab}, path)\n",
    "    # Keep only latest 3\n",
    "    files = sorted(glob.glob(os.path.join(checkpoint_dir, \"mm_audio_*.pt\")), key=os.path.getmtime)\n",
    "    if len(files) > 3:\n",
    "        for f in files[:-3]:\n",
    "            os.remove(f)\n",
    "    print(f\"Saved checkpoint: {path}\")\n",
    "\n",
    "def train(text_corpus, vocab_src, image_corpus, audio_corpus):\n",
    "    vocab        = build_vocab(vocab_src)\n",
    "    tok          = WordTokenizer(vocab)\n",
    "    text_loader  = DataLoader(TextDataset(text_corpus, tok), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    img_loader   = get_image_loader(image_corpus)\n",
    "    audio_loader = get_audio_loader(audio_corpus)\n",
    "\n",
    "    model  = MMDiffuser(tok.vsz).to(DEVICE)\n",
    "    shared = list(model.unet.parameters())\n",
    "    opt    = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    tloss  = nn.NLLLoss(ignore_index=IGNORE_IDX)\n",
    "    iloss  = nn.MSELoss()\n",
    "    aloss  = nn.MSELoss()\n",
    "\n",
    "    print(f\"[i] params: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    global last_save_time\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        ti, ii, ai = iter(text_loader), iter(img_loader), iter(audio_loader)\n",
    "        steps = max(len(text_loader), len(img_loader), len(audio_loader))\n",
    "        for step in range(1, steps+1):\n",
    "            try: xb = next(ti).to(DEVICE)\n",
    "            except StopIteration:\n",
    "                ti = iter(text_loader); xb = next(ti).to(DEVICE)\n",
    "            t_txt    = torch.randint(0, DIFF_STEPS, (xb.size(0),), device=DEVICE)\n",
    "            xb_noisy = q_sample_text(xb, t_txt, tok.mask)\n",
    "            logits   = model.forward_text(xb_noisy, t_txt)\n",
    "            tgt      = xb.clone(); tgt[xb_noisy!=tok.mask] = IGNORE_IDX\n",
    "            lt       = tloss(logits.view(-1, logits.size(-1)), tgt.view(-1))\n",
    "\n",
    "            try: imgs,_ = next(ii)\n",
    "            except StopIteration:\n",
    "                ii = iter(img_loader); imgs,_ = next(ii)\n",
    "            imgs   = imgs.to(DEVICE)\n",
    "            t_img  = torch.randint(0, DIFF_STEPS, (imgs.size(0),), device=DEVICE)\n",
    "            noise  = torch.randn_like(imgs)\n",
    "            imgs_n = q_sample_img(imgs, t_img)\n",
    "            pred_i = model.forward_img(imgs_n, t_img)\n",
    "            li     = iloss(pred_i, noise.view(noise.size(0), -1, noise.shape[-1]))\n",
    "\n",
    "            try: spec,_ = next(ai)\n",
    "            except StopIteration:\n",
    "                ai   = iter(audio_loader); spec,_ = next(ai)\n",
    "            spec    = spec.to(DEVICE)\n",
    "            t_a     = torch.randint(0, DIFF_STEPS, (spec.size(0),), device=DEVICE)\n",
    "            noise_a = torch.randn_like(spec)\n",
    "            spec_n  = q_sample_audio(spec.transpose(1,2), t_a).transpose(1,2)\n",
    "            pred_a  = model.forward_audio(spec_n, t_a)\n",
    "            al      = aloss(pred_a, noise_a.transpose(1,2))\n",
    "\n",
    "            grads_t = torch.autograd.grad(lt, shared, retain_graph=True)\n",
    "            gnorm_t = torch.sqrt(sum((g**2).sum() for g in grads_t))\n",
    "            grads_i = torch.autograd.grad(li, shared, retain_graph=True)\n",
    "            gnorm_i = torch.sqrt(sum((g**2).sum() for g in grads_i))\n",
    "            grads_a = torch.autograd.grad(al, shared, retain_graph=True)\n",
    "            gnorm_a = torch.sqrt(sum((g**2).sum() for g in grads_a))\n",
    "\n",
    "            target = (gnorm_t + gnorm_i + gnorm_a) / 3\n",
    "            w_t = (target / (gnorm_t + 1e-8)).detach()\n",
    "            w_i = (target / (gnorm_i + 1e-8)).detach()\n",
    "            w_a = (target / (gnorm_a + 1e-8)).detach()\n",
    "\n",
    "            loss = w_t*lt + w_i*li + w_a*al\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(f\"ep{ep} step{step:4d}  txt={lt.item():.4f}×{w_t.item():.2f} \"\n",
    "                      f\"img={li.item():.4f}×{w_i.item():.2f} \"\n",
    "                      f\"aud={al.item():.4f}×{w_a.item():.2f}\")\n",
    "            if time.time() - last_save_time > save_interval:\n",
    "                save_checkpoint(model, vocab, checkpoint_dir)\n",
    "                last_save_time = time.time()\n",
    "        save_checkpoint(model, vocab, checkpoint_dir)\n",
    "    print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fdb11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: replace paths with your data locations\n",
    "text_corpus = '/content/data/text'\n",
    "vocab_src   = '/content/data/text'\n",
    "image_corpus= '/content/data/images'\n",
    "audio_corpus= '/content/data/audio'\n",
    "\n",
    "train(text_corpus, vocab_src, image_corpus, audio_corpus)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
